{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import pandas as pd\n",
    "import json\n",
    "import traceback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('./data/computer_science_keywords.json', 'r') as f:\n",
    "    cs_keywords = json.load(f, encoding='utf-8')\n",
    "    keywords_list = list(cs_keywords.keys())\n",
    "    cs_avg = sum(list(cs_keywords.values())) / float(len(keywords_list))\n",
    "    cs_keywords['computer science'] **= 2\n",
    "with open('./data/physics_keywords.json', 'r') as f:    \n",
    "    physics_keywords = json.load(f, encoding='utf-8')\n",
    "    physics_avg = sum(list(physics_keywords.values())) / float(len(keywords_list))\n",
    "    physics_keywords['physics'] **= 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-755.7698440314393 389453379844.0\n"
     ]
    }
   ],
   "source": [
    "print(cs_avg, max(cs_keywords.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cp_ref = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read file 140\n",
      "662311\n",
      "Read file 141\n",
      "661314\n",
      "Read file 142\n",
      "661729\n",
      "Read file 143\n",
      "661509\n",
      "Read file 144\n",
      "661398\n",
      "Read file 145\n",
      "659741\n",
      "Read file 146\n",
      "661275\n",
      "Read file 147\n",
      "661673\n",
      "Read file 148\n",
      "660863\n",
      "Read file 149\n",
      "661513\n",
      "Read file 150\n",
      "661609\n",
      "Read file 151\n",
      "661286\n",
      "Read file 152\n",
      "660812\n",
      "Read file 153\n",
      "661830\n",
      "Read file 154\n",
      "662077\n",
      "Read file 155\n",
      "661524\n",
      "Read file 156\n",
      "661035\n",
      "Read file 157\n",
      "661390\n",
      "Read file 158\n",
      "660997\n",
      "Read file 159\n",
      "661308\n"
     ]
    }
   ],
   "source": [
    "content = []\n",
    "for file in range(140, 160):\n",
    "    print('Read file', file)\n",
    "    with open('./data/mag_papers_%d.txt' % file) as f:\n",
    "        content = f.readlines()\n",
    "    \n",
    "    new_data = {}\n",
    "    for i, line in enumerate(content):\n",
    "        obj = json.loads(line)\n",
    "\n",
    "        if 'authors' not in obj or 'title' not in obj or 'year' not in obj:\n",
    "            continue\n",
    "\n",
    "        # title\n",
    "        title = obj['title'] \n",
    "\n",
    "        # Authors list\n",
    "        authors = []\n",
    "        for author in obj['authors']:\n",
    "            authors.append(author['name'])\n",
    "        authors = json.dumps(authors, ensure_ascii=False)\n",
    "\n",
    "        # n_citation\n",
    "        n_citation = obj['n_citation'] if 'n_citation' in obj else 0\n",
    "\n",
    "        # references list\n",
    "        references = json.dumps(obj['references'], ensure_ascii=False) if 'references' in obj else '[]'\n",
    "\n",
    "        # venue\n",
    "        if 'venue' in obj and len(obj['venue']) > 0:\n",
    "            venue = obj['venue']\n",
    "        elif 'publisher' in obj and len(obj['publisher']) > 0:\n",
    "            venue = obj['publisher']\n",
    "        else:\n",
    "            venue = ''\n",
    "\n",
    "        # year\n",
    "        year = obj['year']\n",
    "\n",
    "        # fos list\n",
    "        if 'fos' in obj:\n",
    "            fos = json.dumps(obj['fos'], ensure_ascii=False).lower()\n",
    "\n",
    "            cs_score = 0\n",
    "            physics_score = 0\n",
    "            is_found = False\n",
    "            for word in obj['fos']:\n",
    "                lword = word.lower()\n",
    "                cs_score += cs_keywords[lword] if lword in cs_keywords else 0\n",
    "                physics_score += physics_keywords[lword] if lword in physics_keywords else 0\n",
    "            if cs_score > cs_avg:\n",
    "                field = 'Computer Science'\n",
    "                is_found = True\n",
    "            elif physics_score > physics_avg:\n",
    "                field = 'Physics'\n",
    "                is_found = True\n",
    "            else:\n",
    "                field = 'N/A'\n",
    "\n",
    "            new_data[obj['id']] = {'authors':authors, 'n_citation': n_citation, 'references':references, 'title':title,\n",
    "                              'venue':venue, 'year':year, 'fos':fos, 'field': field}\n",
    "            if is_found and 'references' in obj:\n",
    "                cp_ref[obj['id']] = obj['references']\n",
    "                \n",
    "    print(len(new_data))\n",
    "    \n",
    "    df = pd.DataFrame.from_dict(new_data, orient='index').reset_index()\n",
    "    df.rename(columns=dict(zip(df.columns[[0]], ['id'])),inplace=True)\n",
    "    df = df.set_index('id')\n",
    "    df.to_csv('./data/mag_%d.csv' % file, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('./data/cp_ref_7.json', 'w') as f:\n",
    "    json.dump(cp_ref, f, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/mag_140.csv').set_index('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n"
     ]
    }
   ],
   "source": [
    "for file in range(141, 160):\n",
    "    print(file)\n",
    "    df1 = pd.read_csv('./data/mag_%d.csv' % file).set_index('id')\n",
    "    df = pd.concat([df, df1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.to_csv('./data/mag_refined7.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove unnecessary ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file 0\n",
      "file 1\n",
      "file 2\n",
      "file 3\n",
      "file 4\n",
      "file 5\n",
      "file 6\n",
      "file 7\n",
      "file 8\n"
     ]
    }
   ],
   "source": [
    "# Get all needed id\n",
    "nece_id = {}\n",
    "for file in range(0, 9):\n",
    "    print('file', file)\n",
    "    with open('./data/cp_ref_%d.json' % file, 'r') as f:\n",
    "        obj = json.load(f, encoding='utf-8')\n",
    "        for idx, iid in enumerate(obj):\n",
    "            nece_id[iid] = True\n",
    "            for rid in obj[iid]:\n",
    "                nece_id[rid] = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with open('./data/all_id.json', 'w') as f:\n",
    "    json.dump(nece_id, f, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('./data/all_id.json', 'r') as f:\n",
    "    nece_id = json.load(f, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 6\n",
      "Read 6 finished\n",
      "Convert to dict\n",
      "0\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "6000000\n",
      "7000000\n",
      "8000000\n",
      "9000000\n",
      "10000000\n",
      "Convert to dataframe\n",
      "Concat\n",
      "length: 3782267\n",
      "To csv finished\n",
      "Read 7\n",
      "Read 7 finished\n",
      "Convert to dict\n",
      "0\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "6000000\n",
      "7000000\n",
      "8000000\n",
      "9000000\n",
      "10000000\n",
      "Convert to dataframe\n",
      "Concat\n",
      "length: 3782513\n",
      "To csv finished\n",
      "Read 8\n",
      "Read 8 finished\n",
      "Convert to dict\n",
      "0\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "Convert to dataframe\n",
      "Concat\n",
      "length: 1171350\n",
      "To csv finished\n"
     ]
    }
   ],
   "source": [
    "for file in range(6, 9):\n",
    "    df = cp_df = na_df = na_dict = cp_ref_dict = cp_ref_df = new_df = None\n",
    "    print('Read', file)\n",
    "    df = pd.read_csv('./data/mag_refined%d.csv' % file, encoding='utf-8').set_index('id').fillna('N/A')\n",
    "    print('Read',file,'finished')\n",
    "    cp_df = df.loc[(df['field'] == 'Computer Science') | (df['field'] == 'Physics')]\n",
    "    na_df = df.loc[(df['field'] != 'Computer Science') & (df['field'] != 'Physics')]\n",
    "    na_dict = na_df.to_dict('index')\n",
    "    print('Convert to dict')\n",
    "    cp_ref_dict = {}\n",
    "    for idx, iid in enumerate(na_dict):\n",
    "        if idx%1000000==0:\n",
    "            print(idx,)\n",
    "        row = na_dict[iid]\n",
    "        if iid in nece_id:\n",
    "            ref_list = json.loads(row['references'])\n",
    "            new_ref_list = []\n",
    "            for ref in ref_list:\n",
    "                if ref in nece_id:\n",
    "                    new_ref_list.append(ref)\n",
    "            row['references'] = json.dumps(new_ref_list, ensure_ascii=False)\n",
    "            cp_ref_dict[iid] = row\n",
    "    print('Convert to dataframe')\n",
    "    cp_ref_df = pd.DataFrame.from_dict(cp_ref_dict, orient='index').reset_index()\n",
    "    cp_ref_df.rename(columns=dict(zip(cp_ref_df.columns[[0]], ['id'])),inplace=True)\n",
    "    cp_ref_df = cp_ref_df.set_index('id')\n",
    "    print('Concat')\n",
    "    new_df = pd.concat([cp_df, cp_ref_df], sort=True)\n",
    "    print('length:',len(new_df))\n",
    "    new_df.to_csv('./data/mag_reduced_%d.csv' % file, encoding='utf-8')\n",
    "    print('To csv finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('./data/all_id.json', 'r') as f:\n",
    "    nece_id = json.load(f, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 6\n",
      "To dict\n",
      "Check references\n",
      "0\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "Convert to dataframe\n",
      "length: 2852357 Prev length 3782267\n",
      "To csv finished\n",
      "Read 7\n",
      "To dict\n",
      "Check references\n",
      "0\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "Convert to dataframe\n",
      "length: 2854833 Prev length 3782513\n",
      "To csv finished\n",
      "Read 8\n",
      "To dict\n",
      "Check references\n",
      "0\n",
      "1000000\n",
      "Convert to dataframe\n",
      "length: 884396 Prev length 1171350\n",
      "To csv finished\n"
     ]
    }
   ],
   "source": [
    "# Refernce reduction\n",
    "for file in range(6, 9):\n",
    "    print('Read', file)\n",
    "    df = pd.read_csv('./data/mag_reduced_%d.csv' % file, encoding='utf-8', dtype={'field':str}).set_index('id')\n",
    "    print('To dict')\n",
    "    dic = df.to_dict('index')\n",
    "    df = None\n",
    "    print('Check references')\n",
    "    new_dic = {}\n",
    "    for idx, iid in enumerate(dic):\n",
    "        print(idx) if idx % 1000000 == 0 else None\n",
    "        # Paper in other fields\n",
    "        if iid not in nece_id:\n",
    "            continue\n",
    "        row = dic[iid]\n",
    "        new_ref = []\n",
    "        ref_list = json.loads(row['references'])\n",
    "        for ref in ref_list:\n",
    "            # Preserve necessary references\n",
    "            if ref in nece_id:\n",
    "                new_ref.append(ref)\n",
    "        row['references'] = json.dumps(new_ref)\n",
    "        new_dic[iid] = row\n",
    "    print('Convert to dataframe')\n",
    "    cp_ref_df = pd.DataFrame.from_dict(new_dic, orient='index').reset_index()\n",
    "    cp_ref_df.rename(columns=dict(zip(cp_ref_df.columns[[0]], ['id'])),inplace=True)\n",
    "    cp_ref_df = cp_ref_df.set_index('id')\n",
    "    print('length:',len(cp_ref_df), 'Prev length', len(dic))\n",
    "    cp_ref_df.to_csv('./data/mag_new_reduced_%d.csv' % file, encoding='utf-8')\n",
    "    print('To csv finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>authors</th>\n",
       "      <th>field</th>\n",
       "      <th>fos</th>\n",
       "      <th>n_citation</th>\n",
       "      <th>references</th>\n",
       "      <th>title</th>\n",
       "      <th>venue</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>f6559ed0-c806-4662-856e-b29232a911bb</th>\n",
       "      <td>[\"Claudia Menghi\", \"Claudia Gatta\", \"Alberto V...</td>\n",
       "      <td>Computer Science</td>\n",
       "      <td>[\"operating system\", \"rev\"]</td>\n",
       "      <td>50</td>\n",
       "      <td>[\"8c0c3545-c16e-497e-87ad-570d6e97d43f\"]</td>\n",
       "      <td>Parasitosis adquirida por consumo de sushi</td>\n",
       "      <td>Revista Argentina De Microbiologia</td>\n",
       "      <td>2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f6559f41-9737-4ee0-9f1e-ace9e11c2153</th>\n",
       "      <td>[\"E. M. Egorova\", \"A. A. Revina\"]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[\"stereochemistry\", \"chemistry\", \"analytical c...</td>\n",
       "      <td>50</td>\n",
       "      <td>[\"af4e3a2a-0f38-4ae7-87e4-0ab4347cf39e\"]</td>\n",
       "      <td>Optical Properties and Sizes of Silver Nanopar...</td>\n",
       "      <td>Colloid Journal</td>\n",
       "      <td>2002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f6559faa-b8f4-41dc-8982-576ffd2a7388</th>\n",
       "      <td>[\"Marco Chacin\", \"Kazuya Yoshida\"]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[\"control engineering\", \"mobile robot\", \"compu...</td>\n",
       "      <td>50</td>\n",
       "      <td>[\"0570b47a-a732-425b-901d-00e2571660dc\", \"0790...</td>\n",
       "      <td>MULTI-LIMBED ROVER FOR ASTEROID SURFACE EXPLOR...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f655a070-0da7-46d6-b95b-6b6c9d20f265</th>\n",
       "      <td>[\"Alejandro Cáceres\", \"Suzanne S. Sindi\", \"Ben...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[\"genome-wide association study\", \"linkage dis...</td>\n",
       "      <td>50</td>\n",
       "      <td>[\"0219395f-e9af-404b-80d3-fcb9f5b43f46\", \"08ab...</td>\n",
       "      <td>Identification of polymorphic inversions from ...</td>\n",
       "      <td>BMC Bioinformatics</td>\n",
       "      <td>2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f655a0bb-ebcf-4d7c-8020-ab1d7b96ab75</th>\n",
       "      <td>[\"Sorin Tunaru\", \"Jukka Kero\", \"Annette Schaub...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[\"biology\", \"endocrinology\", \"biochemistry\", \"...</td>\n",
       "      <td>622</td>\n",
       "      <td>[\"06d26ce5-40a0-42c8-8257-8367c86c4c1b\", \"0933...</td>\n",
       "      <td>PUMA-G and HM74 are receptors for nicotinic ac...</td>\n",
       "      <td>Nature Medicine</td>\n",
       "      <td>2003</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                authors  \\\n",
       "id                                                                                        \n",
       "f6559ed0-c806-4662-856e-b29232a911bb  [\"Claudia Menghi\", \"Claudia Gatta\", \"Alberto V...   \n",
       "f6559f41-9737-4ee0-9f1e-ace9e11c2153                  [\"E. M. Egorova\", \"A. A. Revina\"]   \n",
       "f6559faa-b8f4-41dc-8982-576ffd2a7388                 [\"Marco Chacin\", \"Kazuya Yoshida\"]   \n",
       "f655a070-0da7-46d6-b95b-6b6c9d20f265  [\"Alejandro Cáceres\", \"Suzanne S. Sindi\", \"Ben...   \n",
       "f655a0bb-ebcf-4d7c-8020-ab1d7b96ab75  [\"Sorin Tunaru\", \"Jukka Kero\", \"Annette Schaub...   \n",
       "\n",
       "                                                 field  \\\n",
       "id                                                       \n",
       "f6559ed0-c806-4662-856e-b29232a911bb  Computer Science   \n",
       "f6559f41-9737-4ee0-9f1e-ace9e11c2153               NaN   \n",
       "f6559faa-b8f4-41dc-8982-576ffd2a7388               NaN   \n",
       "f655a070-0da7-46d6-b95b-6b6c9d20f265               NaN   \n",
       "f655a0bb-ebcf-4d7c-8020-ab1d7b96ab75               NaN   \n",
       "\n",
       "                                                                                    fos  \\\n",
       "id                                                                                        \n",
       "f6559ed0-c806-4662-856e-b29232a911bb                        [\"operating system\", \"rev\"]   \n",
       "f6559f41-9737-4ee0-9f1e-ace9e11c2153  [\"stereochemistry\", \"chemistry\", \"analytical c...   \n",
       "f6559faa-b8f4-41dc-8982-576ffd2a7388  [\"control engineering\", \"mobile robot\", \"compu...   \n",
       "f655a070-0da7-46d6-b95b-6b6c9d20f265  [\"genome-wide association study\", \"linkage dis...   \n",
       "f655a0bb-ebcf-4d7c-8020-ab1d7b96ab75  [\"biology\", \"endocrinology\", \"biochemistry\", \"...   \n",
       "\n",
       "                                      n_citation  \\\n",
       "id                                                 \n",
       "f6559ed0-c806-4662-856e-b29232a911bb          50   \n",
       "f6559f41-9737-4ee0-9f1e-ace9e11c2153          50   \n",
       "f6559faa-b8f4-41dc-8982-576ffd2a7388          50   \n",
       "f655a070-0da7-46d6-b95b-6b6c9d20f265          50   \n",
       "f655a0bb-ebcf-4d7c-8020-ab1d7b96ab75         622   \n",
       "\n",
       "                                                                             references  \\\n",
       "id                                                                                        \n",
       "f6559ed0-c806-4662-856e-b29232a911bb           [\"8c0c3545-c16e-497e-87ad-570d6e97d43f\"]   \n",
       "f6559f41-9737-4ee0-9f1e-ace9e11c2153           [\"af4e3a2a-0f38-4ae7-87e4-0ab4347cf39e\"]   \n",
       "f6559faa-b8f4-41dc-8982-576ffd2a7388  [\"0570b47a-a732-425b-901d-00e2571660dc\", \"0790...   \n",
       "f655a070-0da7-46d6-b95b-6b6c9d20f265  [\"0219395f-e9af-404b-80d3-fcb9f5b43f46\", \"08ab...   \n",
       "f655a0bb-ebcf-4d7c-8020-ab1d7b96ab75  [\"06d26ce5-40a0-42c8-8257-8367c86c4c1b\", \"0933...   \n",
       "\n",
       "                                                                                  title  \\\n",
       "id                                                                                        \n",
       "f6559ed0-c806-4662-856e-b29232a911bb         Parasitosis adquirida por consumo de sushi   \n",
       "f6559f41-9737-4ee0-9f1e-ace9e11c2153  Optical Properties and Sizes of Silver Nanopar...   \n",
       "f6559faa-b8f4-41dc-8982-576ffd2a7388  MULTI-LIMBED ROVER FOR ASTEROID SURFACE EXPLOR...   \n",
       "f655a070-0da7-46d6-b95b-6b6c9d20f265  Identification of polymorphic inversions from ...   \n",
       "f655a0bb-ebcf-4d7c-8020-ab1d7b96ab75  PUMA-G and HM74 are receptors for nicotinic ac...   \n",
       "\n",
       "                                                                   venue  year  \n",
       "id                                                                              \n",
       "f6559ed0-c806-4662-856e-b29232a911bb  Revista Argentina De Microbiologia  2007  \n",
       "f6559f41-9737-4ee0-9f1e-ace9e11c2153                     Colloid Journal  2002  \n",
       "f6559faa-b8f4-41dc-8982-576ffd2a7388                                 NaN  2005  \n",
       "f655a070-0da7-46d6-b95b-6b6c9d20f265                  BMC Bioinformatics  2012  \n",
       "f655a0bb-ebcf-4d7c-8020-ab1d7b96ab75                     Nature Medicine  2003  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cp_ref_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonexisted_id = []\n",
    "for iid in new_dic:\n",
    "    if iid not in nece_id:\n",
    "        nonexisted_id.append(iid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nonexisted_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
